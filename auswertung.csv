#,Publication Year,Author,Einordung (Idee),Title,Zusammenfassung,Promptart,Optimierungsmethodik,Untersuchte Anwendungsgebiete,Verbesserung der Modellleistung,Evalutionsmetriken,Generalisierbarkeit in den Studien,# Zitationen,,
1,2024,"Lococ, Damien; Ivanov, Samuel; Kowalski, James; Montoya, Edward",Modellbasierte Methoden,Token-level Optimization for Enhanced Text Generation: A Prompt Engineering Framework with Large Language Models,Entwicklung eines token-level-guided automatic prompt optimization (TAPO) Framework mit Feedback Mechanismus auf Embedding-Ebene.,Soft-Prompt,Anpassungen auf Token-level während Textgenerierung mit Feedback-Loop innerhalb des Modells.,"Textverständnis, Übersetzung, Textgenerierung",Ja,"Perplexity, BLEU Score, Factual Accuracy",Nein,11,,
2,2023,"Zhang, Zhihan; Wang, Shuohang; Yu, Wenhao; Xu, Yichong; Iter, Dan; Zeng, Qingkai; Liu, Yang; Zhu, Chenguang; Jiang, Meng",Modellbasierte Methoden,Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models,"Erzeugen von Kanidaten-Prompts anhand von gegebenem Seed-Prompt. Kanidaten werden geranked und ausgewählt. Ziel ist das optimieren für Prompts bei neuen Einsatzszenarien.

Es wird mit Beispielen gearbeitet. Verschiedene Metaprompts werden getestet Auswahl über Ranking Modell, was an Beispielaufgaben trainiert wird und Prompts für neue Aufgaben ranked. Ranking-Modell wird evaluiert.",Hard-Prompt,"Task-Prompterzeugung durch ChatGPT (Black-Box-Model), Selection durch Fine-Tuned-Modell",Generalisierungsfähigkeit,Ja,"Win Rate (vs Human), Accuracy",Ja,15,,
3,2023,"Pryzant, Reid; Iter, Dan; Li, Jerry; Lee, Yin Tat; Zhu, Chenguang; Zeng, Michael","Gradientenbasierte Methoden, Modellbasierte Methoden","Automatic Prompt Optimization with ""Gradient Descent"" and Beam Search","Gradient Descent mit Prompts und Beam Search für besten Prompt. Nur Black Box Prompting, kein spezielles Modell notwendig. ",Hard-Prompt,Gradient Descent auf Hard-Prompts,Textklassifikation,Ja,F1 Score,Nein,219,,
4,2024,"Yilar, Jonathan; Foster, Olivia; Woods, Benjamin","Modellbasierte Methoden, Augmentation",Recursive In-Context Learning for Autonomous Prompt Generation in Large Language Models: A Self-Instructed Approach,"Prompt Refinement mit In-Context Learning. BLEU, ROUGE, BERTScore werden als Bewertungskriterien für den Prompt genutzt.",Hard-Prompt,LLM generiert neuen Prompt anhand von Feedback.,"Technisches schreiben, Zusammenfassung, Konversationsagent",Ja,"BLEU Score, BERT Score, Diversity Score",Innerhalb der Domäne,16,,
5,2024,"Mañas, Oscar; Astolfi, Pietro; Hall, Melissa; Ross, Candace; Urbanek, Jack; Williams, Adina; Agrawal, Aishwarya; Romero-Soriano, Adriana; Drozdzal, Michal",Modellbasierte Methoden,Improving Text-to-Image Consistency via Automatic Prompt Optimization,LLM erzeugt neue Text zu Bild Prompts anhand von Beispielen und Meta-Prompt,Hard-Prompt,Prompts werden zur Bildgenerierung genutzt. Bild wird Bewertet. Bewertung und Prompt wird in ein LLM zur Optimisierung gegeben,Bildgenerierung,Ja,"Davidsonian Scene Graph score, CLIP Score, FID Score",Nein,15,x,
6,2022,"Zhang, Zhuosheng; Zhang, Aston; Li, Mu; Smola, Alex",Automatisierung bestehender Prompt-Engineering Methoden,Automatic Chain of Thought Prompting in Large Language Models,"Automatisiert Chain of Thought Prompting mit zwei Schritten. Erstens Clustering von Fragen. Danach wähle Fragen aus und generiere Reasoning Chains, welche als Beispiele dienen. Prompt wird danach mit Beispielen erweitert.",Hard-Prompt,"Automatisches Chain of Thought, Fragen Clustering und Auswahl",Argumentation,Ja,Accuracy,Nein,717,,
7,2024,"Wen, Yuxin; Jain, Neel; Kirchenbauer, John; Goldblum, Micah; Geiping, Jonas; Goldstein, Tom",Gradientenbasierte Methoden,Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery,Hard-Prompts werden durch „Inversion“ zu Soft-Prompts. Diese werden Optimisiert in dem sie transformiert und Gradienten bestimmt werden.,"Hard-Prompt, Soft-Prompt",Hard-Prompts werden mit Prompt Inversion und Gradient basierter Optimierung gesucht.,Bildgenerierung,Ja,CLIP Score,Nein,196,,Gute Beschreibung von Hard- und Softprompts
8,2024,"Levi, Elad; Brosh, Eli; Friedmann, Matan",Augmentation,Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases,Erzeugt synthetische Beispiele für In-Context Learning und automatisches Few-Shot-Prompting. Externes Modell zur Prompt bewertung.,Hard-Prompt,Generiert Beispiele für In-Context learning.,"Textklassifikation, Textgenerierung",Ja,Accuracy,Nein,2,,
9,2024,S. Liu; S. Yu; Z. Lin; D. Pathak; D. Ramanan,"Gradientenbasierte Methoden, Modellbasierte Methoden",Language Models as Black-Box Optimizers for Vision-Language Models,Nutzten LLMs zum Generieren von neuen Prompts. Initial werden positive wie negative Beispiele gegeben und bei jedem Schritt eine Bewertung mitgegeben. Modell kann impliziten „Gradienten“ nutzen. CLIP wird als LVM Model zur Bewertung genutzt.,Hard-Prompt,LLM als automatischer Prompt Engineer für VLMs.,"Bildgenerierung, Textklassifikation",Ja,"Accuracy, Faithfulness Score, Likert scale score, ",Nein,22,,
10,2024,F. Ma; X. Liang; M. Tao,Pruning,LinguaShrink: Reducing Token Overhead With Psycholinguistics,Training von Kompressionsmodell. Ziel ist die Kompression von Prompts für kleiner Eingaben mit möglichst wenig Informationsverlust,Hard-Prompt,Prompt Compressor wird trainiert und zum Komprimieren von Prompts genutzt.,"Argumentation, Textverständnis, Übersetzung, Zusammenfassung",Ja,"BLEU Score, ROUGE Score, F1 Score, CSE Score",Nein,0,x,
11,2022,"Chen, Xiang; Zhang, Ningyu; Xie, Xin; Deng, Shumin; Yao, Yunzhi; Tan, Chuanqi; Huang, Fei; Si, Luo; Chen, Huajun",Augmentation,KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction,Prompt-Tuning mit Knowledge und Relations Injection für In-Context Learning. ,Soft-Prompt,Prompt-Tuning mit Knowledge und Relations Injection für In-Context Learning. ,Relationsextrahierung,Ja,F1 Score,Nein,396,,
12,2024,"Ahmed, Toufique; Pai, Kunal Suresh; Devanbu, Premkumar; Barr, Earl",Augmentation,Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization),Automatisches erzeugen von Prompts mit Few-Shot-Beispielen. Nutzt BM25 zum finden von Beispielen.,Hard-Prompt,"Auswahl von Code-Beispielen und Augmentation durch externe Informationen wie Repository, Author usw.","Code Zusammenfassung, Code Vervollständigung",Ja,"BLEU Score, ROUGE Score, METEOR, Exact Match, Edit Similarity",Nein,47,,
13,2024,"Liu, Dairui; Yang, Boming; Du, Honghui; Greene, Derek; Hurley, Neil; Lawlor, Aonghus; Dong, Ruihai; Li, Irene",Modellbasierte Methoden,RecPrompt: A Self-tuning Prompting Framework for News Recommendation Using Large Language Models,Recomender genertiert Prompt und Optimizer refined den Prompt. ,Hard-Prompt,LLM generiert neuen Template prompt.,Recommendations,Ja,"AUC, MRR, nDCG@5 (N@5), N@10, TopicScore",Nein,0,,
14,2024,"Abukhalaf, Seif; Hamdaqa, Mohammad; Khomh, Foutse",Augmentation,PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4,Prompt Templating und Augmentation für UML generierung.,Hard-Prompt,Einfacher RAG-Case mit UML Informationen und Templating.,UML-Generierung,Ja,"Inference Cost, UML Validity, Correctness, McNemars Test",Nein,3,,
15,2023,"Wang, Yunlong; Shen, Shuyuan; Lim, Brian Y",Modellbasierte Methoden,RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions,"Original Prompt soll verändert werden um Emotionen in Bilder besser dazustellen, dabei wird CLIP Score als Bewertung genutzt. Ein Proxy Modell wird zur Abschätzung von Promptqualität trainiert und genutzt. Proxy Modell bewertet Prompt anhand von Text-Features wie #Adjektive etc.",Hard-Prompt,Proxy Modell bewertet Prompt. Prompt wird automatisch auf Verbesserungen angepasst.,Bildgenerierung,Ja,"User studies, Image-Emotion Alignment CLIP Score, image-text alignment",Ja,84,,
16,2024,"Wen, Hao; Li, Yuanchun; Liu, Guohong; Zhao, Shanhui; Yu, Tao; Li, Toby Jia-Jun; Jiang, Shiqi; Liu, Yunhao; Zhang, Yaqin; Liu, Yunxin",Pruning,AutoDroid: LLM-powered Task Automation in Android,Query LLM mit HTML von App. LLM gibt Anweisungen wie weiter verfahren wird. Promp zeit wird verkürzt in dem Aufgaben zusammengefasst werden und durch pruning.,Hard-Prompt,Prompt-Pruning (Kosten und Interferenz Optimierung),UI-Testing,Ja,"Accuracy, Success Rate",Nein,27,,
17,2023,"Li, Lei; Zhang, Yongfeng; Chen, Li",Modellbasierte Methoden,Personalized Prompt Learning for Explainable Recommendation,Fine-Tuning Strategie für White-Box Modelle für Empfehlungsmodelle mit guter Erklärbarkeit.  ,Soft-Prompt,Multi-Stage-Soft-Prompt fine tuning.,Empfehlungen,Ja,"BLEU Score, ROUGE Score, Unique Sentence Ratio, Feature Matching Ratio (FMR), Feature Coverage Ratio (FCR). Feature Diversity (DIV)",Nein,169,,
18,2024,"Zhou, Yujia; Liu, Zheng; Jin, Jiajie; Nie, Jian-Yun; Dou, Zhicheng",Augmentation,Metacognitive Retrieval-Augmented Large Language Models,RAG welcher Meta-Denkprozesse simulieren soll. Mit einem zweiten Modell wird der „Denkprozess“ überwacht.,Hard-Prompt,Metakognitive Denkprozesse durch Expertenmodell.,"Fragenbeantwortung, Argumentation",Ja,"Exact Match, F1, precision, recall, Accuracy",Nein,15,x,
19,2024,"Ukai, Mahiro; Kurita, Shuhei; Hashimoto, Atsushi; Ushiku, Yoshitaka; Inoue, Nakamasa",Pruning,AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering,Erstellt komprimierte Preprompts. In zweiter Phase werden Prompts anhand von Fragenklassifikation gewählt und Programm erstellt. Python APIs werden zum Anreichern des Kontextes eingepflegt,Hard-Prompt,Promptkompression,Codegenerierung,Ja,"Prompt Länge, Accuracy",Ja,0,,
20,2024,"Imazato, Koki; Shimada, Kazutaka",Augmentation,Automatic Few-Shot Selection on In-Context Learning for Aspect Term Extraction,Few-Shot Kanidatenauswahl mit Pre-Trained Language Model. Danach Prompt Augmentation mit den gewählten Beispielen.,Hard-Prompt,Auswahl von Few-Shot Beispielen für In-Context Learning,"Textklassifikation, Aspect Term Extraction",Ja,"Precission, Recall, F1 Score",Nein,0,x,
21,2024,"Wu, Tong; He, Hao",Augmentation,Automatic Prompt Generation Based on Combinatorial Relationships in Reasoning Problems,Nutzt Black-Box LLM zum Argumentieren mit Satzbeziehungen. Beziehungen werden von Fine-Tuned Modell bestimmt,Hard-Prompt,Auswahl von Kombinatorisch relevanten Fakten,Argumentation,Ja,Accuracy,Nein,0,,
22,2024,"Zheng, Yuanhang; Tan, Zhixing; Li, Peng; Liu, Yang",Modellbasierte Methoden,Black-Box Prompt Tuning With Subspace Learning,Optimierung von Deep Continuous Prompts durch Subspace Learning,Soft-Prompt,Optimierung von Deep Continuous Prompts durch Subspace Learning,"Textklassifikation, Übersetzung, Textgenerierung",Ja,"Accuracy, Pearson’s correlation coefficient, BLEU Score, METEOR, ROUGE-L",Ja,8,,
23,2023,"Tanaka, Hiroto; Mori, Naoki; Okada, Makoto","Evolutionäre Algorithmen, Modellbasierte Methoden",Genetic Algorithm for Prompt Engineering with Novel Genetic Operators,Suche nach effektiven Prompts mit Genetische Algorithmen.,Hard-Prompt,Suche nach effektiven Prompts mit Genetische Algorithmen.,Fragenbeantwortung,-,Fitness(Accuracy),Nein,1,,
24,2024,"Clemmer, Colton; Ding, Junhua; Feng, Yunhe",Modellbasierte Methoden,PreciseDebias: An Automatic Prompt Engineering Approach for Generative AI to Mitigate Image Demographic Biases,"Erzeuge mit LLMs neue Prompts, welche Bias umgehen soll. Beispiel mit Ehnik von Krankenschwestern.",Hard-Prompt,Fine-tuned LLM erzeugt neue Prompts.,Bildgenerierung,Ja,"Average Bias %, Success rate",Nein,0,,
25,2024,"Kim, Hannah; Lee, Hyun; Pang, Sunyu; Oh, Uran",Augmentation,Prompirit: Automatic Prompt Engineering Assistance for Improving AI-Generated Art Reflecting User Emotion,Füge Prompts Emotionen oder Styleguides mithilfe von Finetuned-Modellen,Hard-Prompt,Erweitere Prompts um Emotionen durch Fine-Tuned LLM.,Bildgenerierung,Ja,"Image-Emotion Alignment CLIP Score, image-text alignment",Nein,0,,
26,2024,"Taveekitworachai, Pittawat; Abdullah, Febri; Gursesli, Mustafa Can; Lanata, Antonio; Guazzini, Andrea; Thawonmas, Ruck",Augmentation,Prompt Evolution Through Examples for Large Language Models–A Case Study in Game Comment Toxicity Classification,Prompt Enginerring mit Genetischem Algorithmus mit LLM als reasoning tool. Negativ Beispiele für In-Context Learning.,Hard-Prompt,Black-Box LLM erzeugt neuer Prompt anhand von Negativ-Beispielen.,Textklassifikation,Ja,Accuracy,Nein,2,,
27,2024,"Zhu, Yiming; Yin, Zhizhuo; Tyson, Gareth; Haq, Ehsan-Ul; Lee, Lik-Hang; Hui, Pan",Augmentation,APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT,"Prompt-Tuning Pipeline mit drei Schritten. Erst Templating, Prompt Augmentation mit few-shot learning examples und abschließend text metriken fürs „Reasoning“",Hard-Prompt,"Templating, Few-Shot-Learning, Prompt Augmentation durch Metriken wie Toxicity Score usw.",Textklassifikation,Ja,"F1 Score, precision, recall",Nein,1,,"Bin ich aktuell zu dumm für?

TODO"
28,2024,"Ha, Thien-Loc; Ho, Trong-Bao; Nguyen, Long; Dinh, Dien",Automatisierung bestehender Prompt-Engineering Methoden,Auto Graph of Thoughts: A Hands-free and Cost Effective Method for using Graph of Thoughts,Automatisierung von Graph of Thoughts. Sprachmodell wird nur mit vorherigen Ergebnissen und Teilaufgabe zum Prompt-Tuning genutzt. Prompts werden auch genutzt um Scoring zu bekommen. Teilweise menschliches Eingreifen.,Hard-Prompt,"Automatisches Graph of Thoughts, Ranking, Semi-Automatisch","Sortierung, Wortüberschneidung, Wortzählung, Kombinieren von Dokumenten",Ja,Mean Error Score,Nein,0,,
29,2024,"Mu, Fangwen; Shi, Lin; Wang, Song; Yu, Zhuohao; Zhang, Binquan; Wang, ChenXue; Liu, Shichao; Wang, Qing",Augmentation,ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification,Ermögliche von Nachfragen beim Anwender um Anforderungen zu prüfen. Automatisches erzeugen von Seed-Inputs. Few-Shot-Examples. ,Hard-Prompt,Nachfragen beim Nutzer um Prompt mit Antwort zu Erweitern,Codegenerierung,Ja,"Precision, Recall, F1 Score, Accuracy (Pass@1 metric)",Nein,2,Semiautomatic,
30,2024,"Li, Cheng; Zhang, Mingyang; Mei, Qiaozhu; Kong, Weize; Bendersky, Michael",Modellbasierte Methoden,Learning to Rewrite Prompts for Personalized Text Generation,Sehr gute Übersicht zum Thema. Nutzt Reinforcement learning zum Umschreiben von Prompts für personalisierte Textgenerierung. Es wird der originale Prompt randomized für nöchste Generation und an das Black-Box Modell geschickt. Die Ergebnisse werden genutzt um einen rewriter zu trainieren. Mit Reinforment learning wird der Rewriter verbessert.,Hard-Prompt,Prompt Rewriter wird trainiert und zum Umschreiben genutzt.,Textgenerierung,Ja,"BLEU Score, ROUGE Score, Overlap",Nein,7,,
31,2024,"Saletta, Martina; Ferretti, Claudio","Evolutionäre Algorithmen, Modellbasierte Methoden",Exploring the Prompt Space of Large Language Models through Evolutionary Sampling,Suche nach effektiven Prompts mit Sampling.,Hard-Prompt,Suche nach effektiven Prompts mit Sampling.,Argumentation,Ja,Correctness,Ja,1,,
32,2020,"Jiang, Zhengbao; Xu, Frank F.; Araki, Jun; Neubig, Graham",Modellbasierte Methoden,How Can We Know What Language Models Know?,Suche nach Prompts zum extrahieren von Wissen aus Sprachmodellen und Prompt-Selection.,Hard-Prompt,Paraphrasierung und Dependency-basierte Promptgenerierung.,Faktenvorhersage,Ja,Accuracy,Nein,1384,,
33,2021,"Haviv, Adi; Berant, Jonathan; Globerson, Amir",Modellbasierte Methoden,BERTese: Learning to Speak to BERT,Rewriter findet neuen masked Prompt mit trainiertem BERT Model und BERT embedding Dictionary. Gefundene Embeddings werden zu ihrem nearest neighbour zurück transformiert.,Soft-Prompt,Suche nach Soft-Prompts mit Model.,Faktenvorhersage,Ja,Precision,Ja,124,,
34,2020,"Shin, Taylor; Razeghi, Yasaman; IV, Robert L. Logan; Wallace, Eric; Singh, Sameer",Gradientenbasierte Methoden,AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,AutoPrompt kombiniert masking und automatisches Prompt Engineering. Dabei werden Trigger Tokens hinter die eigentliche Eingabe und vor die Maske gesetzt. Die Tokens werden mit Gradienten Basierter Suche gesucht.,Soft-Prompt,Suche nach Trigger Tokens mit Gradienten Suche.,"Textverständnis, Faktenvorhersage, Relationsextrahierung",Ja,"Accuracy, Precision",Nein,1765,,
35,2021,"Li, Xiang Lisa; Liang, Percy",Modellbasierte Methoden,Prefix-Tuning: Optimizing Continuous Prompts for Generation,"Entwurf von Prefixtuning (kontinuierliche Prompts) statt Finetuning für verbesserung von Textgenerierung. Modelparameter müssen nicht angepasst werden. Bessere Performance, wenn wenig Daten vorhanden.",Soft-Prompt,Prefixtuning als Alternative zu Finetuning,"Tabelle zu Textgenerierung, Zusammenfassung",Ja,"BLEU Score, NIST, METEOR, ROUGE, CIDEr, TERM, Mover Score",Nein,3849,,
36,2023,"Liu, Xiao; Zheng, Yanan; Du, Zhengxiao; Ding, Ming; Qian, Yujie; Yang, Zhilin; Tang, Jie",Modellbasierte Methoden,"GPT Understands, Too",P-Tuning als Kombination aus diskreten Prompts und trainierten prompt Embeddings.,Soft-Prompt,Suche nach optimierten Prompt-Embeddings,Textverständnis,Ja,Precision,Nein,1122,,
